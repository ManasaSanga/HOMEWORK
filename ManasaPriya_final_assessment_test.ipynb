{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6wnJGAO41X0"
   },
   "source": [
    "## 1. PANDAS & NUMPY (25 points)\n",
    "\n",
    "- complete 10 questions below\n",
    "- for Pandas exercise, please use provided __sales_reps_data.csv__ file\n",
    "- it would be a good idea to familiarise yourself with the .csv file first. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "id": "6JO2SIlG41X4",
    "outputId": "a2ed0a5b-29b3-4ad0-d077-644c54caf419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date   region sales_rep     item  qty  unit_price    total\n",
      "0   01/06/2019     East     Jones   Pencil   95        1.99   189.05\n",
      "1      1/23/19  Central    Kivell   Binder   50       19.99   999.50\n",
      "2   02/09/2019  Central   Jardine   Pencil   36        4.99   179.64\n",
      "3      2/26/19  Central      Gill      Pen   27       19.99   539.73\n",
      "4      3/15/19     West   Sorvino   Pencil   56        2.99   167.44\n",
      "5   04/01/2019     East     Jones   Binder   60        4.99   299.40\n",
      "6      4/18/19  Central   Andrews   Pencil   75        1.99   149.25\n",
      "7   05/05/2019  Central   Jardine   Pencil   90        4.99   449.10\n",
      "8      5/22/19     West  Thompson   Pencil   32        1.99    63.68\n",
      "9   06/08/2019     East     Jones   Binder   60        8.99   539.40\n",
      "10     6/25/19  Central    Morgan   Pencil   90        4.99   449.10\n",
      "11  07/12/2019     East    Howard   Binder   29        1.99    57.71\n",
      "12     7/29/19     East    Parent   Binder   81       19.99  1619.19\n",
      "13     8/15/19     East     Jones   Pencil   35        4.99   174.65\n",
      "14  09/01/2019  Central     Smith     Desk    2      125.00   250.00\n",
      "15     9/18/19     East     Jones  Pen Set   16       15.99   255.84\n",
      "16  10/05/2019  Central    Morgan   Binder   28        8.99   251.72\n",
      "17    10/22/19     East     Jones      Pen   64        8.99   575.36\n",
      "18  11/08/2019     East    Parent      Pen   15       19.99   299.85\n",
      "19    11/25/19  Central    Kivell  Pen Set   96        4.99   479.04\n",
      "20  12/12/2019  Central     Smith   Pencil   67        1.29    86.43\n",
      "21    12/29/19     East    Parent  Pen Set   74       15.99  1183.26\n",
      "22     1/15/20  Central      Gill   Binder   46        8.99   413.54\n",
      "23  02/01/2020  Central     Smith   Binder   87       15.00  1305.00\n",
      "24     2/18/20     East     Jones   Binder    4        4.99    19.96\n",
      "25  03/07/2020     West   Sorvino   Binder    7       19.99   139.93\n",
      "26     3/24/20  Central   Jardine  Pen Set   50        4.99   249.50\n",
      "27  04/10/2020  Central   Andrews   Pencil   66        1.99   131.34\n",
      "28     4/27/20     East    Howard      Pen   96        4.99   479.04\n",
      "29     5/14/20  Central      Gill   Pencil   53        1.29    68.37\n",
      "30     5/31/20  Central      Gill   Binder   80        8.99   719.20\n",
      "31     6/17/20  Central    Kivell     Desk    5      125.00   625.00\n",
      "32  07/04/2020     East     Jones  Pen Set   62        4.99   309.38\n",
      "33     7/21/20  Central    Morgan  Pen Set   55       12.49   686.95\n",
      "34  08/07/2020  Central    Kivell  Pen Set   42       23.95  1005.90\n",
      "35     8/24/20     West   Sorvino     Desk    3      275.00   825.00\n",
      "36  09/10/2020  Central      Gill   Pencil    7        1.29     9.03\n",
      "37     9/27/20     West   Sorvino      Pen   76        1.99   151.24\n",
      "38    10/14/20     West  Thompson   Binder   57       19.99  1139.43\n",
      "39    10/31/20  Central   Andrews   Pencil   14        1.29    18.06\n",
      "40    11/17/20  Central   Jardine   Binder   11        4.99    54.89\n",
      "41  12/04/2020  Central   Jardine   Binder   94       19.99  1879.06\n",
      "42    12/21/20  Central   Andrews   Binder   28        4.99   139.72\n"
     ]
    }
   ],
   "source": [
    "# 1. load sales_reps_data.csv file with Pandas as dataframe\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sales_reps_data = pd.read_csv(\"sales_reps_data.csv\")\n",
    "print(sales_reps_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yJbfmR7-41X6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows are:\n",
      "         date   region sales_rep    item  qty  unit_price   total\n",
      "0  01/06/2019     East     Jones  Pencil   95        1.99  189.05\n",
      "1     1/23/19  Central    Kivell  Binder   50       19.99  999.50\n",
      "2  02/09/2019  Central   Jardine  Pencil   36        4.99  179.64\n",
      "3     2/26/19  Central      Gill     Pen   27       19.99  539.73\n",
      "4     3/15/19     West   Sorvino  Pencil   56        2.99  167.44\n",
      "last 5 rows are:\n",
      "          date   region sales_rep    item  qty  unit_price    total\n",
      "38    10/14/20     West  Thompson  Binder   57       19.99  1139.43\n",
      "39    10/31/20  Central   Andrews  Pencil   14        1.29    18.06\n",
      "40    11/17/20  Central   Jardine  Binder   11        4.99    54.89\n",
      "41  12/04/2020  Central   Jardine  Binder   94       19.99  1879.06\n",
      "42    12/21/20  Central   Andrews  Binder   28        4.99   139.72\n"
     ]
    }
   ],
   "source": [
    "# 2. print the first and last five rows\n",
    "import pandas as pd\n",
    "df = pd.read_csv('sales_reps_data.csv')\n",
    "result = df.head(5)\n",
    "print(\"First 5 rows are:\")\n",
    "print(result)\n",
    "df = pd.read_csv('sales_reps_data.csv')\n",
    "result = df.tail(5)\n",
    "print(\"last 5 rows are:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "462cQMFu41X7"
   },
   "outputs": [],
   "source": [
    "#3. Find names of the sales reps who sold the most items (qty) and what are those items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeqGkfSp41X7"
   },
   "outputs": [],
   "source": [
    "# 4. Print details(records) for sales rep called 'Jones'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M0pXjfNM41X8"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-14-d69ede55ad70>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-d69ede55ad70>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(\"Highest value: \"); print(df['unit_price'].idxmax())\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# 5. Get each item's highest price\n",
    " print(\"Highest value: \"); print(df['unit_price'].idxmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9eOPaeQT41X9"
   },
   "outputs": [],
   "source": [
    "# 6. Find the average price of each unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YVE_YvQO41X-"
   },
   "outputs": [],
   "source": [
    "# 7. Sort all sales records by 'region' and 'total' columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hyJO-t6F41X-"
   },
   "outputs": [],
   "source": [
    "# 8.  Using NumPy create a 4X3 integer array from a range between 200 to 440  \n",
    "# such that the difference between each element is 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gv8q4L2v41X_"
   },
   "outputs": [],
   "source": [
    "# 9. Create a new array like --> array([  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100])\n",
    "# then multiply every element in the array by 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjegAYjs41YA"
   },
   "outputs": [],
   "source": [
    "# 10. Find the unique elements in an array [5,5,11,11,2,3,4,8,14,14,15,5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq9jBWax41YA"
   },
   "source": [
    "## 2. VISUALIZATION MATPLOTLIB (25 points / 15 points)\n",
    "\n",
    "\n",
    "#### 25 points\n",
    "- use already familiar __sales_reps_data.csv__ file \n",
    "- you need to process your data using grouby manupilation to prepare it for plotting (follow prompts below).\n",
    "- you can see how the data should be shaped by checking __sales_resp_data_aggregated.csv__\n",
    "- build a Pie Chart as described below. \n",
    "\n",
    "#### 15 points\n",
    "- if you are not able to perform groupby operations to prepare data for plotting, then use __sales_resp_data_aggregated.csv__ file.\n",
    "- build a Pie Chart as described below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nWQqx6041YB"
   },
   "outputs": [],
   "source": [
    "# 1. load  sales_reps_data.csv file to a dataframe\n",
    "\n",
    "\n",
    "# 2. Prepare a new subset from the original data, aggregating sales records by sales representative and summing up\n",
    "# their total amount of sales (check sales_resp_data_aggregated.csv file to see what needs to be achieved)\n",
    "\n",
    "\"\"\"\n",
    "HINT: How to rename a column? \n",
    "\n",
    " - sometimes when you group by column A and sum up colum B values, you lose the name of column B\n",
    " - below is the example of how to name the column B after group by\n",
    " - it is the equivalent of SQL: SELECT columnA, sum(columnB) as 'Total'\n",
    "\n",
    "Example:\n",
    "  new_df = <your groupby and sum code goes here>.reset_index(name ='total_amount')\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZHZ4lhL41YB"
   },
   "outputs": [],
   "source": [
    "# 3. Using matplotlib create a Pie plot to show proportion of sales contribution to the total sales by each sales rep. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIH5EWOf41YC"
   },
   "source": [
    "## 3. MACHINE LEARNING QUESTIONS (25 points)\n",
    "\n",
    "- this is purely theoretical task, NO CODING required\n",
    "- answer 5 questions below using Markdown cells to write your answers\n",
    "- note that you only need to provide brief answers to each question in your own words and examples where applicable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPi3gtsq41YC"
   },
   "source": [
    "\n",
    "### Q1: What is Machine Leanring?\n",
    "A stream of research of artificial intelligence (AI) and computer science called machine learning . It mainly focuses on using data and algorithms to simulate how humans learn, gradually increasing the accuracy of the system.\n",
    "\n",
    "Machine learning is important because it allows businesses see patterns in customer activity, operational behaviour and facilitates the development of new products. Machine learning is a key component of the operations of many of today's top corporations, including Facebook, Google, and Uber. For many businesses, machine learning now significantly differentiates them from their competitors.\n",
    "\n",
    "\n",
    "### Q2: What are the key principles of 'Supervised' Machine learning?\n",
    "The key principle behind supervised learning models is that we construct them using data that contains values for all of the model's inputs and outputs.\n",
    "\n",
    "We require a data collection with information on income and the other demographic parameters for a large number of people in order to create a supervised learning model for an income problem.\n",
    "\n",
    "A supervised learning model will investigate the correlation between each individual's input and output values.\n",
    "\n",
    "After that, it creates a mathematical formula that connects the inputs and output.\n",
    "\n",
    "\n",
    "### Q3: With regards to Machine Learning Model Fitting, what do we mean my Underfitting and Overfitting?\n",
    "Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms.\n",
    "Overfitting in machine learning:\n",
    "When a model fits the training data too well, it is said to be overfit.\n",
    "When a model learns the information and noise in the training data to the point where it adversely affects the model's performance on fresh data, this is known as overfitting. This indicates that the machine learns concepts from the noise or random oscillations in the training data. These ideas don't apply to fresh data, which poses a difficulty for the models' capacity to generalise.\n",
    "\n",
    "Nonparametric and nonlinear models, which have more flexibility while learning a target function, are more likely to exhibit overfitting. To limit and constrain how much detail the model learns, several nonparametric machine learning algorithms also contain parameters or strategies.\n",
    "\n",
    "For instance, decision trees are a nonparametric machine learning technique that can overfit training data and is very versatile. After a tree learns, it can be pruned to remove part of the detail it has absorbed to solve this problem.\n",
    "\n",
    "Underfitting in machine learning\n",
    "A model is said to be underfit when it is unable to both model the training data and generalise to new data.\n",
    "\n",
    "A machine learning model that is underfit is not appropriate and will be visible since it will perform poorly on the training data.\n",
    "\n",
    "Since underfitting is simple to identify with a strong performance metric, it is frequently not emphasised. Moving on and experimenting with other machine learning methods is the solution. However, it does offer a useful counterpoint to the overfitting issue.\n",
    "\n",
    "\n",
    "### Q4: Explain how do we ususally split data in preparation for the Machine Learning process?\n",
    "The Purpose of splitting data into the different category is to avoid overfitting\n",
    "\n",
    "Ethically, it is suggested to divide your dataset into three parts to avoid overfitting and model selection bias called -\n",
    "\n",
    "1.Training set (Has to be the largest set)\n",
    "2.Cross-Validation set or Development set or Dev set\n",
    "3.Testing Set\n",
    "\n",
    "1. Training set\n",
    "The actual subset of the dataset that we utilise to train the model, or the sample of data that was used to fit the model (estimating the weights and biases in the case of Neural Network). This data is observed, the model learns from it, and its parameters are optimised.\n",
    "2. Cross validation set\n",
    "We select the appropriate model or the degree of the polynomial (if using regression model only) by minimizing the error on the cross-validation set.\n",
    "3.test set\n",
    "the subset of data that was used to impartially assess how well a final model fit the training dataset. Once the model has fully trained using the training and validation sets, it is only used. Therefore, the test set is used to simulate the kind of circumstance that will be encountered after the model is made available for use in real-time.\n",
    "\n",
    "How the data is splited into ratios\n",
    "\n",
    "The key goal of choosing the splitting ratio is to ensure that the overall trend of our original dataset is present in all three sets. It is feasible that we will choose a model that is skewed towards the patterns found solely in the dev set if our dev set contains very little data. The same is true for training sets; too little data will cause the model to be biassed toward certain trends that are unique to that subset of the dataset.\n",
    "\n",
    "The models we use are merely estimators that are trained to recognise statistical patterns in the data. Therefore, it is crucial that the statistical distributions of the data used to learn and those used to validate or test the model are as close as possible. Choosing the subsets — in this case, the training set, the development set, and/or the test set — at random is one approach to accomplish this as perfectly as you can. The properties of the train set and the dev/test set will not match, for instance, if you are working on a face identification project and the dev/test images were acquired from a user's cell phone while the training images were taken from the web.\n",
    "\n",
    "Using the train test split method twice is one technique to partition the dataset into the train, test, and cv with 0.6, 0.2, and 0.2 ratios:\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "x, x_test, y, y_test = train_test_split (x_train,labels, test_size=0.2, train_size=0.8 )\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x,y,test_size = 0.25, train_size =0.75)\n",
    "\n",
    "### Q5: What is SciKit?\n",
    "Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. Through a consistent Python interface, it offers a variety of effective methods for statistical modelling and machine learning, including classification, regression, clustering, and dimensionality reduction. This Python-based library is based on NumPy, SciPy, and Matplotlib.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nUEQXfc41YD"
   },
   "source": [
    "## 4. DATA ANALYSIS NARRATIVE (25 points)\n",
    "\n",
    "- this is purely theoretical task, NO CODING required\n",
    "- please use provided PDF file DATA_FINAL_ASSESSMENT_Q4.pdf with the actual task descritpion and graphs /plots\n",
    "- write your answers here in the Jyputer Notebook using a markdown cell. \n",
    " \n",
    " 1. Data sample(Head call)\n",
    "It can be observed that Men's who are  21 years old and women's who are 23 years old are having the highest spending score and in contrast to them who's age is less than the 20 in both genders are having the least spending score.\n",
    "\n",
    "2. Gender Distribution\n",
    "In this graph it is clearly illustrated as female population is higher than male population who visits to the shopping centre.\n",
    "\n",
    "3. Customer ages\n",
    "In this graph it can be observed that people who are of age 31 marked as the most visited age groups compared to others. Where as people of age 19 and 49 are the second most visited age groups in addition to that people who are of age 55 marked as the least visited customers.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "-7558200954517678561final_assessment_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
